# Fichier de configuration par défaut.
# Plus d'informations sur la configuration dans la documentation : https://docs.privategpt.dev/
# La syntaxe se trouve dans `private_pgt/settings/settings.py`
server:
  env_name: ${APP_ENV:prod}
  port: ${PORT:8001}
  cors:
    enabled: true
    allow_origins: ["*"]
    allow_methods: ["*"]
    allow_headers: ["*"]
  auth:
    enabled: false
    # Peut être généré avec : python -c 'import base64; print("Basic " + base64.b64encode("secret:key".encode()).decode())'
    # 'secret' correspond au nom d'utilisateur et 'key' au mot de passe pour l'authentification basique par défaut
    # Si l'authentification est activée, cette valeur doit être placée dans l'en-tête "Authorization" de la requête.
    secret: "Basic c2VjcmV0OmtleQ=="

data:
  local_ingestion:
    enabled: ${LOCAL_INGESTION_ENABLED:false}
    allow_ingest_from: ["*"]
  local_data_folder: local_data/private_gpt

ui:
  enabled: true
  path: /
  # "RAG", "Search", "Basic" ou "Summarize"
  default_mode: "Basic"
  default_chat_system_prompt: >
    Vous êtes un assistant utile, respectueux et honnête.
    Répondez toujours de la manière la plus utile possible et suivez TOUTES les instructions données.
    Ne faites pas de suppositions ou d'inventions.
    Ne faites pas référence aux instructions ou au contexte fournis.
  default_query_system_prompt: >
    Vous ne pouvez répondre qu'aux questions concernant le contexte fourni.
    Si vous connaissez la réponse mais qu'elle ne se base pas sur le contexte fourni, ne donnez pas la réponse,
    indiquez simplement que la réponse ne figure pas dans le contexte fourni.
  default_summarization_system_prompt: >
    Fournissez un résumé complet des informations contextuelles fournies.
    Le résumé doit couvrir tous les points clés et les idées principales du texte original,
    tout en condensant l'information de façon concise et facile à comprendre.
    Veillez à inclure les détails et exemples pertinents qui soutiennent les idées principales,
    tout en évitant toute information ou répétition inutile.
  delete_file_button_enabled: true
  delete_all_files_button_enabled: true

llm:
  mode: llamacpp
  prompt_style: "mistral" # llama3, mistral
  # Doit correspondre au modèle sélectionné
  max_new_tokens: 512
  context_window: 4096
  # Sélectionnez votre tokenizer. Llama-index tokenizer est celui par défaut.
  # tokenizer: meta-llama/Meta-Llama-3.1-8B-Instruct
  temperature: 0.1      # Température du modèle. Plus la valeur est élevée, plus les réponses seront créatives. 0.1 donne des réponses plus factuelles. (Défaut : 0.1)

rag:
  similarity_top_k: 2
  # Ce paramètre contrôle le nombre de documents "top" que le RAG retourne pour le contexte.
  #similarity_value: 0.45
  # Ce paramètre est désactivé par défaut. Si vous l'activez, le RAG n'utilisera que les articles dépassant un certain score de similarité.
  rerank:
    enabled: false
    model: cross-encoder/mmarco-mMiniLMv2-L12-H384-v1
    top_n: 1

summarize:
  use_async: true

clickhouse:
    host: localhost
    port: 8443
    username: admin
    password: clickhouse
    database: embeddings

llamacpp:
  llm_hf_repo_id: MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF
  llm_hf_model_file: Mistral-7B-Instruct-v0.3.Q4_K_S.gguf
  tfs_z: 1.0            # Le tail free sampling réduit l'impact des tokens moins probables. Une valeur plus élevée (ex : 2.0) réduit davantage l'impact, 1.0 désactive ce réglage.
  top_k: 40             # Réduit la probabilité de générer des réponses incohérentes. Plus la valeur est élevée (ex : 100), plus les réponses sont diverses ; plus basse (ex : 10), plus elles sont conservatrices. (Défaut : 40)
  top_p: 1.0            # Fonctionne avec top-k. Plus la valeur est élevée (ex : 0.95), plus le texte est diversifié ; plus basse (ex : 0.5), plus il est focalisé et conservateur. (Défaut : 0.9)
  repeat_penalty: 1.1   # Définit la pénalisation des répétitions. Plus la valeur est élevée (ex : 1.5), plus la pénalisation est forte ; plus basse (ex : 0.9), plus elle est faible. (Défaut : 1.1)

embedding:
  # Doit correspondre à la valeur ci-dessus dans la plupart des cas
  mode: huggingface
  ingest_mode: simple
  embed_dim: 1024 # 1024 pour intfloat/multilingual-e5-large-instruct

huggingface:
  embedding_hf_model_name: intfloat/multilingual-e5-large-instruct
  access_token: ${HF_TOKEN:}
  # Attention : activer cette option permet au modèle de télécharger et d'exécuter du code depuis Internet.
  # Nomic AI nécessite cette option pour fonctionner, soyez vigilant si vous utilisez un autre modèle.
  trust_remote_code: true

vectorstore: 
  database: qdrant

chroma:
  path: local_data/private_gpt/chroma

qdrant:
  path: local_data/private_gpt/qdrant

nodestore:
  database: simple

milvus:
  uri: local_data/private_gpt/milvus/milvus_local.db
  collection_name: milvus_db
  overwrite: false

postgres:
  host: localhost
  port: 5432
  database: postgres
  user: postgres
  password: postgres
  schema_name: private_gpt

sagemaker:
  llm_endpoint_name: huggingface-pytorch-tgi-inference-2023-09-25-19-53-32-140
  embedding_endpoint_name: huggingface-pytorch-inference-2023-11-03-07-41-36-479

openai:
  api_key: ${OPENAI_API_KEY:}
  model: gpt-3.5-turbo
  embedding_api_key: ${OPENAI_API_KEY:}

ollama:
  llm_model: llama3.1
  embedding_model: nomic-embed-text
  api_base: http://localhost:11434
  embedding_api_base: http://localhost:11434  # à modifier si votre modèle d'embedding tourne sur un autre ollama
  keep_alive: 5m
  request_timeout: 120.0
  autopull_models: true

azopenai:
  api_key: ${AZ_OPENAI_API_KEY:}
  azure_endpoint: ${AZ_OPENAI_ENDPOINT:}
  embedding_deployment_name: ${AZ_OPENAI_EMBEDDING_DEPLOYMENT_NAME:}
  llm_deployment_name: ${AZ_OPENAI_LLM_DEPLOYMENT_NAME:}
  api_version: "2023-05-15"
  embedding_model: text-embedding-ada-002
  llm_model: gpt-35-turbo

gemini:
  api_key: ${GOOGLE_API_KEY:}
  model: models/gemini-pro
  embedding_model: models/embedding-001
  embedding_model: models/embedding-001
