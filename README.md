# ETV-GPT

Ce projet est bas√© sur le repo [Private-GPT](https://github.com/zylon-ai/private-gpt).

La documentation compl√®te de private-gpt est disponible sur ce [lien](https://docs.privategpt.dev/overview/welcome/introduction).

## Pr√©-requis
- **Windows 10/11** : Ce projet est con√ßu pour fonctionner sur Windows.
- **Python 3.10+** : Assurez-vous d'avoir Python install√©.
- **Hugging Face Token** : Un token Hugging Face est n√©cessaire pour t√©l√©charger les mod√®les. Vous pouvez en obtenir un [ici](https://huggingface.co/settings/tokens).

## üöÄ Installation rapide

Pour installer les d√©pendances, configurer le projet et activer le support GPU, ex√©cutez les scripts suivants dans cet ordre depuis PowerShell¬†:

```powershell
# 1. Installer les d√©pendances (pyenv, poetry, make, etc.)
.\install_dep.ps1

# 2. Configurer le projet (Python, HuggingFace, d√©pendances)
.\setup.ps1

# 3. (Optionnel) Activer le support GPU pour llama-cpp
.\setup_gpu.ps1
```

## ‚öôÔ∏è Configuration
La configuration du projet se trouve dans le fichier `settings.yaml`. Vous pouvez modifier les param√®tres selon vos besoins, notamment les mod√®les de langage, les chemins d'acc√®s aux donn√©es et les options de RAG (Retrieval-Augmented Generation).

### Principales options de configuration (`settings.yaml`)

- **Mod√®le de langage (LLM)**
  - `llm.mode` : backend utilis√© (`llamacpp`, `openai`, etc.)
  - `llamacpp.llm_hf_repo_id` : d√©p√¥t Hugging Face du mod√®le (ex: `MaziyarPanahi/Mistral-Small-Instruct-2409-GGUF`)
  - `llamacpp.llm_hf_model_file` : fichier du mod√®le √† utiliser (ex: `Mistral-Small-Instruct-2409.Q8_0.gguf`)
  - `llm.prompt_style` : style de prompt (`mistral`, `llama3`, etc.)
  - `llm.max_new_tokens` / `llm.context_window` : taille des r√©ponses et fen√™tre de contexte

- **Embeddings**
  - `embedding.mode` : backend d'embedding (`huggingface`, etc.)
  - `huggingface.embedding_hf_model_name` : mod√®le d'embedding Hugging Face (ex: `BAAI/bge-multilingual-gemma2`)
  - `embedding.embed_dim` : dimension des embeddings (ex: 1024)

- **Base de donn√©es vectorielle**
  - `vectorstore.database` : type de base (`qdrant`, `chroma`, etc.)
  - `qdrant.path` / `chroma.path` : chemin de stockage local

- **RAG (Retrieval-Augmented Generation)**
  - `rag.similarity_top_k` : nombre de documents contextuels √† r√©cup√©rer
  - `rag.rerank.enabled` : activer le reranking des documents

- **UI**
  - `ui.enabled` : activer/d√©sactiver l'interface web
  - `ui.default_mode` : mode par d√©faut (`Basic`, `RAG`, etc.)

- **Authentification**
  - `server.auth.enabled` : activer l'authentification API
  - `server.auth.secret` : cl√© d'authentification

Adaptez ces param√®tres selon vos besoins pour personnaliser le comportement du projet.

√Ä chaque modification du fichier `settings.yaml`, il est recommand√© d'ex√©cuter la commande :
```batch
poetry run python scripts\setup
```

## üí° Ex√©cution du projet
Pour ex√©cuter le projet, utilisez le script suivant :

```batch
poetry run python scripts\setup
make run
```

Une fois le projet lanc√©, vous pouvez acc√©der √† l'interface web si elle est activ√©e, √† l'adresse suivante : [http://localhost:8001](http://localhost:8001).
8001 √©tant le port par d√©faut, vous pouvez le modifier dans `settings.yaml` si n√©cessaire.