# ETV-GPT

Ce projet est bas√© sur le repo [Private-GPT](https://github.com/zylon-ai/private-gpt).

La documentation compl√®te de private-gpt est disponible sur ce [lien](https://docs.privategpt.dev/overview/welcome/introduction).

## Pr√©-requis

- **Windows 10/11** : Ce projet est con√ßu pour fonctionner sur Windows
- **Python 3.10+** : Assurez-vous d'avoir Python install√©
- **Hugging Face Token** (optionnel) : N√©cessaire uniquement si vous devez t√©l√©charger des mod√®les depuis Hugging Face. Si les mod√®les sont d√©j√† t√©l√©charg√©s localement, ce token n'est pas requis. Vous pouvez en obtenir un [ici](https://huggingface.co/settings/tokens)

## Installation

### 1. Installation des d√©pendances
Ex√©cutez les scripts suivants dans cet ordre depuis PowerShell :

```powershell
# 1. Installer les d√©pendances syst√®me (pyenv, poetry, make, etc.)
.\install_dep.ps1

# 2. Configurer le projet (Python, HuggingFace, d√©pendances)
.\setup.ps1

# 3. (Optionnel) Activer le support GPU pour llama-cpp
.\setup_gpu.ps1
```

### 2. Configuration initiale
Apr√®s l'installation, configurez votre token Hugging Face si n√©cessaire :

```batch
huggingface-cli login
```

## Configuration

La configuration du projet se trouve dans le fichier `settings.yaml`. Voici les principales options √† personnaliser :

### Mod√®le de langage (LLM)
- `llm.mode` : Backend utilis√© (`llamacpp`, `openai`, etc.)
- `llamacpp.llm_hf_repo_id` : D√©p√¥t Hugging Face du mod√®le
- `llamacpp.llm_hf_model_file` : Fichier du mod√®le √† utiliser
- `llm.prompt_style` : Style de prompt (`mistral`, `llama3`, etc.)
- `llm.max_new_tokens` / `llm.context_window` : Taille des r√©ponses et fen√™tre de contexte

### Embeddings
- `embedding.mode` : Backend d'embedding (`huggingface`, etc.)
- `huggingface.embedding_hf_model_name` : Mod√®le d'embedding Hugging Face (ex: `BAAI/bge-multilingual-gemma2`)
- `embedding.embed_dim` : Dimension des embeddings (ex: 1024)

### Base de donn√©es vectorielle
- `vectorstore.database` : Type de base (`qdrant`, `chroma`, etc.)
- `qdrant.path` / `chroma.path` : Chemin de stockage local

### RAG (Retrieval-Augmented Generation)
- `rag.similarity_top_k` : Nombre de documents contextuels √† r√©cup√©rer
- `rag.rerank.enabled` : Activer le reranking des documents

### Interface utilisateur
- `ui.enabled` : Activer/d√©sactiver l'interface web
- `ui.default_mode` : Mode par d√©faut (`Basic`, `RAG`, etc.)

### Authentification
- `server.auth.enabled` : Activer l'authentification API
- `server.auth.secret` : Cl√© d'authentification

> [!IMPORTANT]
> √Ä chaque modification du fichier `settings.yaml`, ex√©cutez :
> ```batch
> poetry run python scripts\setup
> ```

## üîß Gestion des mod√®les

### Utiliser un mod√®le local existant
Si vous avez d√©j√† un mod√®le t√©l√©charg√© localement, modifiez `settings.yaml` :

```yaml
llamacpp:
  llm_hf_repo_id: none
  llm_hf_model_file: NomModele.gguf  # Nom du fichier du mod√®le local
```

### T√©l√©charger un nouveau mod√®le depuis Hugging Face
Si vous souhaitez t√©l√©charger un mod√®le depuis Hugging Face, suivez ces √©tapes :
1. Assurez-vous que votre token Hugging Face est configur√© :
   ```batch
   huggingface-cli login
   ```

2. Modifiez `settings.yaml` :
   ```yaml
   llamacpp:
     llm_hf_repo_id: Utilisateur/NomDuModele  # D√©p√¥t Hugging Face
     llm_hf_model_file: NomModele.gguf        # Nom du fichier
   ```

## Lancement du projet

Pour d√©marrer le projet :

```batch
.\start.bat
```

### Acc√®s √† l'interface
- **Interface web** : Disponible √† [http://localhost:8001](http://localhost:8001) uniquement si `ui.enabled` est activ√© dans `settings.yaml`
- **API** : Accessible m√™me si l'interface web est d√©sactiv√©e
- **Port** : Modifiable dans `settings.yaml` (8001 par d√©faut)

## Documentation suppl√©mentaire

Pour plus d'informations d√©taill√©es, consultez la [documentation officielle de Private-GPT](https://docs.privategpt.dev/overview/welcome/introduction).